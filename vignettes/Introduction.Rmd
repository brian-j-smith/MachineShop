---
title: "Introduction to the MachineShop Package"
author: "Brian J Smith"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Introduction to the MachineShop Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 4,
  fig.align = "center"
)

library(kableExtra)

set.seed(123)
```


## The MachineShop Package

`MachineShop` is a meta-package for statistical and machine learning with a common interface for model fitting, prediction, performance assessment, and presentation of results.  Support is provided for predictive modeling of numerical, categorical, and censored time-to-event outcomes and for resample (bootstrap, cross-validation, and split training-test sets) estimation of model performance.  This vignette introduces the package interface with a survival data analysis example, followed by applications to other types of response variables, supported methods of model specification and data preprocessing, and a list of all currently available models.


## Model Fitting and Prediction

The `Melanoma` dataset from the `MASS` package [@andersen:1993:SMB] contains time, in days, to (1) death from disease, (2) alive at end of study, or (3) death from other causes for 205 Denmark patients with malignant melanomas.  Also provided are potential predictors of the survival outcomes.  We begin by loading the `MachineShop`, `survival`, and `MASS` packages required for the analysis as well as the `magrittr` package [@bache:2014:MFP] for its pipe (`%>%`) operator to simplify some of the code syntax.  The dataset is split into a training set to which a survival model will be fit and a test set on which to make predictions.  A global formula `fo` relates the predictors on the right hand side to the overall survival outcome on the left and will be used in all of the survival models in this vignette example. 

```{r}
## Load libraries for the survival analysis
library(MachineShop)
library(survival)
library(MASS)
library(magrittr)

## Malignant melanoma cancer dataset
head(Melanoma)

## Create training and test sets
n <- nrow(Melanoma) * 2 / 3
train <- head(Melanoma, n)
test <- head(Melanoma, -n)

## Global formula for the analysis
fo <- Surv(time, status != 2) ~ sex + age + year + thickness + ulcer
```

Generalized boosted regression models are a tree-based ensemble method that can applied to survival outcomes.  They are available in the `MachineShop` with the function `GBMModel`.  A call to the function creates an instance of the model containing any user-specified model parameters and internal machinery for model fitting, prediction, and performance assessment.  Created models can be supplied to the `fit` function to estimate a relationship (`fo`) between predictors and an outcome based on a set of data (`train`).  The importance of variables in a model fit is estimated with the `varimp` function and plotted with `plot`.  Variable importance is a measure of the relative importance of predictors in a model and has a default range of 0 to 100, where 0 denotes the least important variables and 100 the most.

```{r}
## Fit a generalized boosted model
gbmfit <- fit(fo, data = train, model = GBMModel)

## Predictor variable importance
(vi <- varimp(gbmfit))

plot(vi)
```

From the model fit, predictions are obtained at 2, 5, and 10 years as survival probabilities (`type = "prob"`) and as 0-1 death indicators (default: `type = "response"`).

```{r}
## Predict survival probabilities and outcomes at specified follow-up times
times <- 365 * c(2, 5, 10)
predict(gbmfit, newdata = test, times = times, type = "prob") %>% head

predict(gbmfit, newdata = test, times = times) %>% head
```

A call to `performance` with observed and predicted outcomes will produce model performance metrics.  The metrics produced will depend on the type of the observed variable.  In this case of a `Surv` variable, the metrics are area under the ROC curve [@heagerty:2004:TDR] and Brier score [@graf:1999:ACP] at the specified times and overall time-integrated averages.

```{r}
## Model performance metrics
obs <- response(fo, test)
pred <- predict(gbmfit, newdata = test, times = times, type = "prob")
performance(obs, pred, times = times)
```


## Resample Estimation of Model Performance

Performance of a model can be estimated with resampling methods that simulate repeated training and test set fits and prediction.  Performance metrics are computed on each resample to produce an empirical distribution for inference.  Resampling is controlled in the `MachineShop` with the functions:

BootControl
  : Simple bootstrap resampling.  Models are fit with bootstrap resampled training sets and used to predict the full data set.

CVControl
  : Repeated K-fold cross-validation.  The full data set is repeatedly partitioned into K-folds.  Within a partitioning, prediction is performed on each of the K folds with models fit on all remaining folds.

OOBControl
  : Out-of-bootstrap resampling.  Models are fit with bootstrap resampled training sets and used to predict the unsampled cases.
  
SplitControl
  : Split training and test sets.  The data are randomly partitioned into a training and test set.
  
TrainControl
  : Training resubstitution.  A model is fit on and used to predict the full training set to estimate training, or apparent, error.
  
In our example, performance of models to predict survival at 2, 5, and 10 years will be estimated with five repeats of 10-fold cross-validation.  Variable `metrics` is defined for the purpose of reducing the printed and plotted output in this vignette to only the time-integrated ROC and Brier metrics.  Such subsetting of output would not be done in practice if there is interest in seeing all metrics.

```{r}
## Control parameters for repeated K-fold cross-validation
control <- CVControl(
  folds = 10,
  repeats = 5,
  surv_times = 365 * c(2, 5, 10)
)

## Metrics of interest
metrics <- c("ROC.mean", "Brier.mean")
```


### Parallel Computing

Resampling is implemented with the `foreach` package [@microsoft:2017:FPF] and will run in parallel if a compatible backend is loaded, such as that provided by the `doParallel` package [@microsoft:2017:DFP].

```{r}
library(doParallel)
registerDoParallel(cores = 2)
```


### Resampling for One Model

Resampling of a single model is performed with the `resample` function applied to a model object (e.g. `GBMModel`) and a control object like the one defined previously (`control`).  Summary statistics and plots can be obtained with the `summary` and `plot` functions.

```{r}
## Resample estimation
(res <- resample(fo, data = Melanoma, model = GBMModel, control = control))

summary(res)

plot(res, metrics = metrics)
```

### Resampling for Model Comparisons

Resampled metrics from different models can be combined for comparison with the `Resamples` function.  Names given on the left hand side of the equal operators in the call to `Resamples` will be used as labels in output from the `summary` and `plot` functions.  For these types of model comparisons, the same control structure should be used in all associated calls to `resample` to ensure that resulting model metrics are computed on the same resampled training and test sets.

```{r}
## Resample estimation
gbmres1 <- resample(fo, data = Melanoma, model = GBMModel(n.trees = 25), control = control)
gbmres2 <- resample(fo, data = Melanoma, model = GBMModel(n.trees = 50), control = control)
gbmres3 <- resample(fo, data = Melanoma, model = GBMModel(n.trees = 100), control = control)

## Combine resamples for comparison
(res <- Resamples(GBM1 = gbmres1, GBM2 = gbmres2, GBM3 = gbmres3))

summary(res)[, , metrics]

plot(res, metrics = metrics)
plot(res, metrics = metrics, type = "density")
plot(res, metrics = metrics, type = "errorbar")
plot(res, metrics = metrics, type = "violin")
```

Pairwise model differences for each metric can be calculated with the `diff` function applied to results from a call to `Resamples`.  The differences can be summarized descriptively with the `summary` and `plot` functions and assessed for statistical significance with the `t.test` function.

```{r}
## Pairwise model comparisons
(perfdiff <- diff(res))

summary(perfdiff)[, , metrics]

plot(perfdiff, metrics = metrics)
t.test(perfdiff)[, , metrics]
```


### Model Tuning

Modelling functions may have arguments that define parameters in their model fitting algorithms.  For example, `GBMModel` has arguments `n.trees`, `interaction.dept`, and `n.minobsinnode` that defined the number of decision trees to fit, the maximum depth of variable interactions, and the minimum number of observations in the trees terminal nodes.  The `tune` function is available to fit a model over a grid of parameters and return the model whose parameters provide the optimal fit.  Note that the function name `GBMModel`, and not the function call `GBMModel()`, is supplied as the first argument to `tune`.  Summary statistics and plots of performance across all tuning parameters are available with the `summary` and `plot` functions.

```{r}
## Tune over a grid of model parameters
(gbmtune <- tune(fo, data = Melanoma, model = GBMModel,
                 grid = expand.grid(n.trees = c(25, 50, 100),
                                    interaction.depth = 1:3,
                                    n.minobsinnode = c(5, 10)),
                 control = control))

summary(gbmtune)[, , metrics]

plot(gbmtune, type = "line", metrics = metrics)
```

The value returned by `tune` contains an object produced by a call to the modelling function with the the optimal tuning parameters.  Thus, the value can be passed on to the `fit` function for model fitting to a set of data.

```{r}
## Fit the tuned model
gbmfit <- fit(fo, data = Melanoma, model = gbmtune)
(vi <- varimp(gbmfit))

plot(vi)
```


### Ensemble Models

Ensemble methods combine multiple base learning algorithms as a strategy to improve predictive performance.  Two ensemble methods implemented in `Machineshop` are *stacked regression* [@breiman:1996:SR] and *super learners* [@vanderLann:2007:SL].  Stacked regression fits a linear combination of resampled predictions from specified base learners; whereas, super learners fit a specified model, such as `GBMModel`, to the base learner predictions and optionally also to the original predictor variables.  Illustrated below is a performance evaluation of stacked regression and a super learner fit to gradient boosted, conditional forest, and Lasso-based Cox regression base learners.  In the latter case, a separate gradient boosted model is used as the super learner by default.

```{r}
## Stacked regression
stackedres <- resample(fo, data = Melanoma,
                       model = StackedModel(GBMModel, CForestModel, GLMNetModel(lambda = 0.1)))
summary(stackedres)

## Super learner
superres <- resample(fo, data = Melanoma,
                     model = SuperModel(GBMModel, CForestModel, GLMNetModel(lambda = 0.1)))
summary(superres)
```

### Partial Dependence Plots

Partial dependence plots display the marginal effects of predictors on the response variable.  The response scale displayed in the plots will depend on the response type; i.e. probabilities for factor, original scale for numeric, and proportional risk for Surv types.

```{r results = "hide"}
pd <- dependence(gbmfit, select = c(thickness, age))
plot(pd)
```


### Calibration Curves

Agreement between model-predicted and observed values can be visualized with calibration curves.  In the construction of these curves, cases are partitioned into bins according to their (resampled) predicted responses.  Mean observed responses are then calculated within each of the bins and plotted on the vertical axis against the bin midpoints on the horizontal axis.  Calibration curves that are close to the 45-degree line indicate close agreement between observed and predicted responses and a model that is said to be well calibrated. 

```{r results = "hide"}
cal <- calibration(res)
plot(cal, se = TRUE)
```


### Lift Curves

Lift curves depict the rate at which observed binary responses are identifiable from (resampled) predicted response probabilities.  They are constructed by first sorting predicted responses in descending order.  Then, the cumulative percent of positive responses (true positive findings) is plotted against the cumulative number of cases (positive test rates) in the ordering.  Accordingly, the curve represents the rate at which positive responses are found as a function of the positive test rate among cases.

```{r}
## Requires a binary outcome
fo_surv5 <- factor(time > 365 * 5) ~ sex + age + year + thickness + ulcer
df_surv5 <- subset(Melanoma, status != 2)

res_surv5 <- resample(fo_surv5, data = df_surv5, model = GBMModel)
lf <- lift(res_surv5)
plot(lf, find = 75)
```


## Response Variable Types

### Categorical

Categorical responses with two or more levels should be code as a `factor` variable for analysis.  The metrics returned will depend on the number of factor levels.  Metrics for factors with two levels are as follows.

Accuracy
  : Proportion of correctly classified responses.

Kappa
  : Cohen's kappa statistic measuring relative agreement between observed and predicted classifications.

Brier
  : Brier score.

ROCAUC
  : Area under the ROC curve.

PRAUC
  : Area under the precision-recall curve.
  
Sensitivity
  : Proportion of correctly classified values in the second factor level.
  
Specificity
  : Proportion of correctly classified values in the first factor level.

Index
  : A tradeoff function of sensitivity and specificity as defined by `cutoff_index` in the resampling control functions (default: Sensitivity + Specificity).  The function allows for specification of tradeoffs [@perkins:2006:IOC] other than the default of Youden's J statistic [@youden:1950:IRD].
  
Brier, ROCAUC, and PRAUC are computed directly on predicted class probabilities.  The others are computed on predicted class membership.  Memberships are defined to be in the second factor level if predicted probabilities are greater than a cutoff value defined in the resampling control functions (default: `cutoff = 0.5`).

```{r}
### Pima Indians diabetes statuses (2 levels)
library(MASS)
res <- resample(factor(type) ~ ., data = Pima.tr, model = GBMModel)
summary(res)
```

Metrics for factors with more than two levels are as described below.

Accuracy
  : Proportion of correctly classified responses.

Kappa
  : Cohen's kappa statistic measuring relative agreement between observed and predicted classifications.

WeightedKappa
  : Weighted Cohen's kappa with equally spaced weights.  This metric is only computed for ordered factor responses.
  
Brier
  : Multi-class Brier score.
  
CrossEntropy
  : Average cross entropy loss.
  
Brier and CrossEntropy are computed directly on predicted class probabilities.  The others are computed on predicted class membership, defined as the factor level with the highest predicted probability.

```{r}
### Iris flowers species (3 levels)
res <- resample(factor(Species) ~ ., data = iris, model = GBMModel)
summary(res)
```


### Numerical

Numerical responses should be coded as a `numeric` variable.  Associated performance metrics are as defined below and illustrated with Boston housing price data [@venables:2002:MAS].

R2
  : One minus residual divided by total sums of squares,
  $$R^2 = 1 - \frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i - \bar{y})^2},$$
  where $y_i$ and $\hat{y}_i$ are the $n$ observed and predicted responses.

RMSE
  : Root mean square error,
  $$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n(y_i - \hat{y}_i)^2}.$$
  
MAE
  : Median absolute error,
  $$MAE = \operatorname{median}(|y_i - \operatorname{median}(y)|).$$

```{r}
### Boston housing prices
library(MASS)
res <- resample(medv ~ ., data = Boston, model = GBMModel)
summary(res)
```


### Survival

Survival responses should be coded as a `Surv` variable.  In addition to the ROC and Brier survival metrics described earlier in the vignette, the concordance index [@harrell:1982:EYM] can be obtained if follow-up times are not specified for the prediction.

```{r}
## Censored melanoma cancer survival times
library(survival)
res <- resample(Surv(time, status != 2) ~ ., data = Melanoma, model = GBMModel)
summary(res)
```


## Model Specifications

Model specification here refers to the relationship between the response and predictor variables and the data used to estimate it.  Three main types of specification are supported by the `fit`, `resample`, and `tune` functions: formulas, model frames, and recipes.

### Formulas

Models may be specified with the traditional formula and data frame pair, as was done in the previous examples.  In this specification, in-line functions, interactions, and `.` substitution of variables not already appearing in the formula may be include.

```{r}
## Formula specification
gbmfit <- fit(medv ~ ., data = Boston, model = GBMModel)
varimp(gbmfit)
```

### Model Frames

The second specification is similar to the first, except the formula and data frame pair are give in a `ModelFrame`.  The model frame approach has a few subtle advantages.  One is that cases with missing values on any of the response or predictor variables are excluded from the model frame by default.  This is often desirable for models that cannot handle missing values.  Note, however, that some models like `GBMModel` do accommodate missing values.  For those, missing values can be retained in the model frame by setting its argument `na.action = na.pass`.  A second advantage is user-specification of a factor for stratified resampling via the `strata` argument.

```{r}
## Model frame specification
mf <- ModelFrame(medv ~ ., data = Boston, strata = medv)
gbmfit <- resample(mf, model = GBMModel)
summary(gbmfit)
```

A third is that case weights can be included in the model frame and will be passed on to the model fitting functions.

```{r}
## Model frame specification with case weights
mf <- ModelFrame(ncases / (ncases + ncontrols) ~ agegp + tobgp + alcgp,
                 data = esoph, weights = ncases + ncontrols)
gbmfit <- fit(mf, model = GBMModel)
varimp(gbmfit)
```


### Preprocessing Recipes

The `recipes` package [@kuhn:2018:RPT] provides a framework for defining predictor and response variables and preprocessing steps to be applied to them prior to model fitting.  Using recipes helps to ensure that estimation of predictive performance accounts for all modeling step.  They are also a very convenient way of consistently applying preprocessing to new data.  Stratified resampling and case weights are supported for recipes via the designations of `"case_strata"` and `"case_weight"` roles, respectively.  Recipes currently support `factor` and `numeric` responses, but not generally `Surv`.

```{r}
## Recipe specification
library(recipes)
rec <- recipe(medv ~ ., data = Boston) %>%
  add_role(medv, new_role = "case_strata") %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors())

gbmfit <- resample(rec, model = GBMModel)
summary(gbmfit)
```


### Available Models

Currently available model functions are summarized in the table below according to the types of response variables with which each model can be used.  The package additionally supplies a generic `MLModel` function for users to create their own custom models.

```{r echo = FALSE}
info <- modelinfo()
types <- c("binary" = "b", "factor" = "f", "matrix" = "m", "numeric" = "n",
           "ordered" = "o", "Surv" = "S")
x <- lapply(names(info), function(modelname) {
  c(modelname, ifelse(names(types) %in% info[[modelname]]$types, types, NA))
})
df <- as.data.frame(do.call(rbind, x), stringsAsFactors = FALSE)
names(df) <- c("Constructor", names(types))

toString2 <- function(x) toString(na.omit(x))
df_classes <- data.frame(
  Method = sapply(info, getElement, name = "label"),
  Constructor = df$Constructor,
  Categorical = apply(df[c("binary", "factor", "ordered")], 1, toString2),
  Continuous = apply(df[c("matrix", "numeric")], 1, toString2),
  Survival = apply(df["Surv"], 1, toString2)
)
names(df_classes)[-(1:2)] <- paste0(names(df_classes)[-(1:2)],
                                    footnote_marker_number(1:3))

kable(df_classes, align = c("l", "c", "c", "c", "c"), row.names = FALSE,
      escape = FALSE) %>%
  kable_styling("striped", full_width = FALSE, position = "center") %>%
  add_header_above(c(" " = 1, " " = 1, "Response Variable Types" = 3)) %>%
  footnote(number = c("b = binary, f = factor, o = ordered",
                      "m = matrix, n = numeric",
                      "S = Surv"))
```


## References
